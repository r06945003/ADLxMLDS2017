{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "import string\n",
    "from random import randint\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.seq2seq as seq2seq\n",
    "\n",
    "from tensorflow.python.ops.rnn_cell import GRUCell\n",
    "from tensorflow.python.ops.rnn_cell import LSTMCell\n",
    "from tensorflow.python.ops.rnn_cell import MultiRNNCell\n",
    "from tensorflow.python.ops.rnn_cell import DropoutWrapper, ResidualWrapper\n",
    "\n",
    "from tensorflow.python.ops import array_ops\n",
    "from tensorflow.python.ops import control_flow_ops\n",
    "from tensorflow.python.framework import constant_op\n",
    "from tensorflow.python.framework import dtypes\n",
    "from tensorflow.python.layers.core import Dense\n",
    "from tensorflow.python.util import nest\n",
    "\n",
    "from tensorflow.contrib.seq2seq.python.ops import attention_wrapper\n",
    "from tensorflow.contrib.seq2seq.python.ops import beam_search_decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1450\n",
      "6061\n",
      "6061\n"
     ]
    }
   ],
   "source": [
    "path = \"../../data/MLDS_hw2_data/\"\n",
    "traindir = \"training_data/feat/\"\n",
    "\n",
    "trainfiles = os.listdir(path+traindir)\n",
    "\n",
    "traindata = {}\n",
    "for i in range(0,len(trainfiles)):\n",
    "    traindata[str.split(trainfiles[i],\".\")[0]+'.'+str.split(trainfiles[i],\".\")[1]] = np.load(path+traindir+trainfiles[i])\n",
    "print(len(traindata))\n",
    "\n",
    "trainjsonfile = open(path+\"training_label.json\",\"r\")\n",
    "\n",
    "trainjson = json.load(trainjsonfile)\n",
    "\n",
    "\n",
    "words = []\n",
    "maxlen = -1\n",
    "for x in trainjson:\n",
    "    for y in x['caption']:\n",
    "        y = ''.join(c for c in y if c not in string.punctuation)\n",
    "        ss = str.split(y,\" \")\n",
    "        if len(ss)>maxlen:\n",
    "            maxlen = len(ss)\n",
    "            maxlenStr = y\n",
    "        for z in ss:\n",
    "            words.append(z.lower())\n",
    "encodeWords = {}\n",
    "counter = 4\n",
    "\n",
    "for x in words:\n",
    "    if x not in encodeWords:\n",
    "        encodeWords[x] = counter\n",
    "        counter = counter + 1 \n",
    "encodeWords[\"<PAD>\"] = 0      \n",
    "encodeWords[\"<BOS>\"] = 1\n",
    "encodeWords[\"<EOS>\"] = 2\n",
    "encodeWords[\"<NAN>\"] = 3\n",
    "print(len(encodeWords))\n",
    "\n",
    "decodeWords = {}\n",
    "for key, value in encodeWords.items():\n",
    "    decodeWords[value] = key\n",
    "print(len(decodeWords))\n",
    "\n",
    "np.save(\"encodeWords.npy\",encodeWords)\n",
    "np.save(\"decodeWords.npy\",decodeWords)\n",
    "\n",
    "max_seq_length = 41\n",
    "\n",
    "def getStr(ints):\n",
    "    sentence = ' '.join([decodeWords[int] for int in ints])\n",
    "    sentence = sentence.replace('<BOS> ','').replace(' <EOS>', '')\n",
    "    return sentence\n",
    "\n",
    "def getMiniDataSets():\n",
    "    x_data = np.zeros((1450,80,4096),dtype=\"float32\")\n",
    "    x_label = np.zeros((1450,max_seq_length),dtype=\"int32\")\n",
    "    x_label_train = np.zeros((1450,max_seq_length),dtype=\"int32\")\n",
    "    y_length = np.zeros((1450),dtype=\"int32\")\n",
    "    y_length_train = np.zeros((1450),dtype=\"int32\")\n",
    "\n",
    "    i = 0\n",
    "    for x in trainjson:\n",
    "        name = x[\"id\"]\n",
    "        temp = traindata[name]\n",
    "        counter2 = 0\n",
    "        \n",
    "        random = randint(0, len(x[\"caption\"])-1)\n",
    "        \n",
    "        y = x[\"caption\"][random]\n",
    "\n",
    "        x_data[i] = temp\n",
    "\n",
    "        x_label_temp = []\n",
    "        x_label_train_temp = []\n",
    "        \n",
    "        y = ''.join(c for c in y if c not in string.punctuation)\n",
    "        temp = [encodeWords[x.lower()] for x in str.split(y,\" \")]\n",
    "        \n",
    "        if(len(temp)>20):\n",
    "            temp = temp[:20]\n",
    "        \n",
    "        x_label_temp = temp + [encodeWords[\"<EOS>\"]]\n",
    "        x_label_train_temp = [encodeWords[\"<BOS>\"]] + temp \n",
    "        \n",
    "        y_length[i] = len(x_label_temp)\n",
    "        y_length_train[i] = len(x_label_train_temp) \n",
    "\n",
    "        for xa in range(len(x_label_temp),max_seq_length):\n",
    "            x_label_temp.append(encodeWords[\"<PAD>\"])\n",
    "        for xa in range(len(x_label_train_temp),max_seq_length):\n",
    "            x_label_train_temp.append(encodeWords[\"<PAD>\"])                       \n",
    "            \n",
    "        x_label_temp = np.reshape(x_label_temp,(max_seq_length))\n",
    "        x_label_train_temp = np.reshape(x_label_train_temp,(max_seq_length))\n",
    "        \n",
    "        x_label[i] = x_label_temp\n",
    "        x_label_train[i] = x_label_train_temp \n",
    "        i = i+1\n",
    "        \n",
    "    x_data = np.split(x_data,29)\n",
    "    x_label = np.split(x_label,29)\n",
    "    x_label_train = np.split(x_label_train,29)\n",
    "    y_length = np.split(y_length,29)\n",
    "    y_length_train = np.split(y_length_train,29)\n",
    "    return x_data, x_label, x_label_train, y_length, y_length_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 80, 256)\n"
     ]
    }
   ],
   "source": [
    "#tensorflow   \n",
    "\n",
    "unit = 256\n",
    "inputs = tf.placeholder(tf.float32,[None,80,4096]) \n",
    "labels = tf.placeholder(tf.int32,[None,max_seq_length])\n",
    "labels_train = tf.placeholder(tf.int32,[None,max_seq_length])\n",
    "length = tf.placeholder(tf.int32,[None])\n",
    "length_train = tf.placeholder(tf.int32,[None])\n",
    "batch_size = tf.shape(inputs)[0]\n",
    "sequence_length = tf.fill([batch_size], max_seq_length)\n",
    "\n",
    "def lstm_cell():\n",
    "  return tf.contrib.rnn.BasicLSTMCell(unit)\n",
    "\n",
    "encoder_cell = tf.contrib.rnn.MultiRNNCell([lstm_cell() for _ in range(2)])\n",
    "\n",
    "encoder_outputs, encoder_state = tf.nn.dynamic_rnn(encoder_cell, inputs, dtype=tf.float32)\n",
    "print(encoder_outputs.get_shape())\n",
    "\n",
    "attention_mechanism = tf.contrib.seq2seq.LuongAttention(num_units=unit, memory=encoder_outputs)\n",
    "\n",
    "decoder_cell = tf.contrib.rnn.MultiRNNCell([lstm_cell() for _ in range(2)])\n",
    "attention_cell = tf.contrib.seq2seq.AttentionWrapper(decoder_cell, attention_mechanism)\n",
    "\n",
    "initial_state = attention_cell.zero_state(dtype=tf.float32, batch_size=batch_size)\n",
    "initial_state = initial_state.clone(cell_state=encoder_state) \n",
    "\n",
    "embedding = tf.Variable(tf.random_uniform([len(encodeWords), unit], -0.1, 0.1, dtype=tf.float32))\n",
    "labels_embedded = tf.nn.embedding_lookup(embedding, labels_train)\n",
    "\n",
    "output_projection_layer = Dense(len(encodeWords), use_bias=False)\n",
    "\n",
    "#train\n",
    "helper = tf.contrib.seq2seq.TrainingHelper(labels_embedded, sequence_length)\n",
    "#helper = tf.contrib.seq2seq.ScheduledOutputTrainingHelper(labels_embedded, length_train,  0.5)\n",
    "decoder = tf.contrib.seq2seq.BasicDecoder(attention_cell, helper, initial_state, output_layer=output_projection_layer)\n",
    "\n",
    "decoder_outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(decoder, maximum_iterations=max_seq_length)\n",
    "\n",
    "outputs = decoder_outputs.rnn_output\n",
    "sample = decoder_outputs.sample_id\n",
    "\n",
    "masks = tf.cast(tf.sequence_mask(length, maxlen=max_seq_length),tf.float32);\n",
    "loss = tf.contrib.seq2seq.sequence_loss(logits=outputs, targets=labels, weights=masks,average_across_timesteps=False,average_across_batch=True)\n",
    "loss = tf.reduce_sum(loss)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.001)\n",
    "minimize = optimizer.minimize(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 122.008422 <EOS>\n",
      "1 113.889323 <EOS>\n"
     ]
    }
   ],
   "source": [
    "trainCount = 0\n",
    "totalLoss = 0\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "for j in range(2000):\n",
    "    x_data, x_label, x_label_train, y_length, y_length_train = getMiniDataSets()\n",
    "    for i in range(29):\n",
    "        trainCount = trainCount + 1\n",
    "        \n",
    "        _,l,predict = sess.run([minimize, loss, sample], \n",
    "                               feed_dict={\n",
    "                                   inputs: x_data[i], \n",
    "                                   labels: x_label[i], \n",
    "                                   labels_train: x_label_train[i], \n",
    "                                   length: y_length[i],\n",
    "                                   length_train: y_length_train[i]\n",
    "                               })\n",
    "        \n",
    "        totalLoss += l\n",
    "    ran = randint(0,49)\n",
    "    log = \"%d %f %s\"%(j, totalLoss/trainCount, getStr(predict[ran]))\n",
    "    print(log)\n",
    "    if j%100==0 and j!=0:\n",
    "        saver = tf.train.Saver()\n",
    "        saver.save(sess, \"model\"+str(j)+\".ckpt\")\n",
    "        \n",
    "saver = tf.train.Saver()\n",
    "saver.save(sess, \"model\"+str(j)+\".ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
