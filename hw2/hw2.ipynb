{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "import string\n",
    "from random import randint\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.seq2seq as seq2seq\n",
    "\n",
    "from tensorflow.python.ops.rnn_cell import GRUCell\n",
    "from tensorflow.python.ops.rnn_cell import LSTMCell\n",
    "from tensorflow.python.ops.rnn_cell import MultiRNNCell\n",
    "from tensorflow.python.ops.rnn_cell import DropoutWrapper, ResidualWrapper\n",
    "\n",
    "from tensorflow.python.ops import array_ops\n",
    "from tensorflow.python.ops import control_flow_ops\n",
    "from tensorflow.python.framework import constant_op\n",
    "from tensorflow.python.framework import dtypes\n",
    "from tensorflow.python.layers.core import Dense\n",
    "from tensorflow.python.util import nest\n",
    "\n",
    "from tensorflow.contrib.seq2seq.python.ops import attention_wrapper\n",
    "from tensorflow.contrib.seq2seq.python.ops import beam_search_decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1450\n",
      "6061\n",
      "6061\n"
     ]
    }
   ],
   "source": [
    "path = \"../../data/MLDS_hw2_data/\"\n",
    "traindir = \"training_data/feat/\"\n",
    "\n",
    "trainfiles = os.listdir(path+traindir)\n",
    "\n",
    "traindata = {}\n",
    "for i in range(0,len(trainfiles)):\n",
    "    traindata[str.split(trainfiles[i],\".\")[0]+'.'+str.split(trainfiles[i],\".\")[1]] = np.load(path+traindir+trainfiles[i])\n",
    "print(len(traindata))\n",
    "\n",
    "trainjsonfile = open(path+\"training_label.json\",\"r\")\n",
    "\n",
    "trainjson = json.load(trainjsonfile)\n",
    "\n",
    "\n",
    "words = []\n",
    "maxlen = -1\n",
    "for x in trainjson:\n",
    "    for y in x['caption']:\n",
    "        y = ''.join(c for c in y if c not in string.punctuation)\n",
    "        ss = str.split(y,\" \")\n",
    "        if len(ss)>maxlen:\n",
    "            maxlen = len(ss)\n",
    "            maxlenStr = y\n",
    "        for z in ss:\n",
    "            words.append(z.lower())\n",
    "encodeWords = {}\n",
    "counter = 4\n",
    "\n",
    "for x in words:\n",
    "    if x not in encodeWords:\n",
    "        encodeWords[x] = counter\n",
    "        counter = counter + 1 \n",
    "encodeWords[\"<PAD>\"] = 0      \n",
    "encodeWords[\"<BOS>\"] = 1\n",
    "encodeWords[\"<EOS>\"] = 2\n",
    "encodeWords[\"<NAN>\"] = 3\n",
    "print(len(encodeWords))\n",
    "\n",
    "decodeWords = {}\n",
    "for key, value in encodeWords.items():\n",
    "    decodeWords[value] = key\n",
    "print(len(decodeWords))\n",
    "\n",
    "np.save(\"encodeWords.npy\",encodeWords)\n",
    "np.save(\"decodeWords.npy\",decodeWords)\n",
    "\n",
    "max_seq_length = 41\n",
    "\n",
    "def getStr(ints):\n",
    "    sentence = ' '.join([decodeWords[int] for int in ints])\n",
    "    sentence = sentence.replace('<BOS> ','').replace(' <EOS>', '')\n",
    "    return sentence\n",
    "\n",
    "def getMiniDataSets():\n",
    "    x_data = np.zeros((1450,80,4096),dtype=\"float32\")\n",
    "    x_label = np.zeros((1450,max_seq_length),dtype=\"int32\")\n",
    "    x_label_train = np.zeros((1450,max_seq_length),dtype=\"int32\")\n",
    "    y_length = np.zeros((1450),dtype=\"int32\")\n",
    "    y_length_train = np.zeros((1450),dtype=\"int32\")\n",
    "\n",
    "    i = 0\n",
    "    for x in trainjson:\n",
    "        name = x[\"id\"]\n",
    "        temp = traindata[name]\n",
    "        counter2 = 0\n",
    "        \n",
    "        random = randint(0, len(x[\"caption\"])-1)\n",
    "        \n",
    "        y = x[\"caption\"][random]\n",
    "\n",
    "        x_data[i] = temp\n",
    "\n",
    "        x_label_temp = []\n",
    "        x_label_train_temp = []\n",
    "        \n",
    "        y = ''.join(c for c in y if c not in string.punctuation)\n",
    "        temp = [encodeWords[x.lower()] for x in str.split(y,\" \")]\n",
    "        \n",
    "        if(len(temp)>20):\n",
    "            temp = temp[:20]\n",
    "        \n",
    "        x_label_temp = temp + [encodeWords[\"<EOS>\"]]\n",
    "        x_label_train_temp = [encodeWords[\"<BOS>\"]] + temp \n",
    "        \n",
    "        y_length[i] = len(x_label_temp)\n",
    "        y_length_train[i] = len(x_label_train_temp) \n",
    "\n",
    "        for xa in range(len(x_label_temp),max_seq_length):\n",
    "            x_label_temp.append(encodeWords[\"<PAD>\"])\n",
    "        for xa in range(len(x_label_train_temp),max_seq_length):\n",
    "            x_label_train_temp.append(encodeWords[\"<PAD>\"])                       \n",
    "            \n",
    "        x_label_temp = np.reshape(x_label_temp,(max_seq_length))\n",
    "        x_label_train_temp = np.reshape(x_label_train_temp,(max_seq_length))\n",
    "        \n",
    "        x_label[i] = x_label_temp\n",
    "        x_label_train[i] = x_label_train_temp \n",
    "        i = i+1\n",
    "        \n",
    "    x_data = np.split(x_data,29)\n",
    "    x_label = np.split(x_label,29)\n",
    "    x_label_train = np.split(x_label_train,29)\n",
    "    y_length = np.split(y_length,29)\n",
    "    y_length_train = np.split(y_length_train,29)\n",
    "    return x_data, x_label, x_label_train, y_length, y_length_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 80, 256)\n"
     ]
    }
   ],
   "source": [
    "#tensorflow   \n",
    "\n",
    "unit = 256\n",
    "inputs = tf.placeholder(tf.float32,[None,80,4096]) \n",
    "labels = tf.placeholder(tf.int32,[None,max_seq_length])\n",
    "labels_train = tf.placeholder(tf.int32,[None,max_seq_length])\n",
    "length = tf.placeholder(tf.int32,[None])\n",
    "length_train = tf.placeholder(tf.int32,[None])\n",
    "batch_size = tf.shape(inputs)[0]\n",
    "sequence_length = tf.fill([batch_size], max_seq_length)\n",
    "\n",
    "def lstm_cell():\n",
    "  return tf.contrib.rnn.BasicLSTMCell(unit)\n",
    "\n",
    "encoder_cell = tf.contrib.rnn.MultiRNNCell([lstm_cell() for _ in range(2)])\n",
    "\n",
    "encoder_outputs, encoder_state = tf.nn.dynamic_rnn(encoder_cell, inputs, dtype=tf.float32)\n",
    "print(encoder_outputs.get_shape())\n",
    "\n",
    "attention_mechanism = tf.contrib.seq2seq.LuongAttention(num_units=unit, memory=encoder_outputs, normalize=True)\n",
    "\n",
    "decoder_cell = tf.contrib.rnn.MultiRNNCell([lstm_cell() for _ in range(2)])\n",
    "attention_cell = tf.contrib.seq2seq.AttentionWrapper(decoder_cell, attention_mechanism)\n",
    "\n",
    "initial_state = attention_cell.zero_state(dtype=tf.float32, batch_size=batch_size)\n",
    "initial_state = initial_state.clone(cell_state=encoder_state) \n",
    "\n",
    "embedding = tf.Variable(tf.random_uniform([len(encodeWords), unit], -0.1, 0.1, dtype=tf.float32))\n",
    "labels_embedded = tf.nn.embedding_lookup(embedding, labels_train)\n",
    "\n",
    "output_projection_layer = Dense(len(encodeWords), use_bias=False)\n",
    "\n",
    "#train\n",
    "helper = tf.contrib.seq2seq.TrainingHelper(labels_embedded, sequence_length)\n",
    "#helper = tf.contrib.seq2seq.ScheduledOutputTrainingHelper(labels_embedded, length_train,  0.5)\n",
    "decoder = tf.contrib.seq2seq.BasicDecoder(attention_cell, helper, initial_state, output_layer=output_projection_layer)\n",
    "\n",
    "decoder_outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(decoder, maximum_iterations=max_seq_length)\n",
    "\n",
    "outputs = decoder_outputs.rnn_output\n",
    "sample = decoder_outputs.sample_id\n",
    "\n",
    "masks = tf.cast(tf.sequence_mask(length, maxlen=max_seq_length),tf.float32);\n",
    "loss = tf.contrib.seq2seq.sequence_loss(logits=outputs, targets=labels, weights=masks,average_across_timesteps=False,average_across_batch=True)\n",
    "loss = tf.reduce_sum(loss)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.001)\n",
    "minimize = optimizer.minimize(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 122.602210 <EOS>\n",
      "1 114.137578 <EOS>\n",
      "2 108.971672 <EOS>\n",
      "3 108.154094 <EOS>\n",
      "4 105.875720 <EOS>\n",
      "5 104.606832 <EOS>\n",
      "6 104.202955 <EOS>\n",
      "7 103.668129 <EOS>\n",
      "8 103.269249 <EOS>\n",
      "9 102.775201 <EOS>\n",
      "10 102.283216 <EOS>\n",
      "11 101.546440 <EOS>\n",
      "12 101.238425 a is\n",
      "13 100.592279 <EOS> is is\n",
      "14 100.267562 a is is is\n",
      "15 99.931310 a is is is\n",
      "16 99.667986 <EOS> is is is\n",
      "17 99.172215 a is is is\n",
      "18 98.826725 a is is is is is\n",
      "19 98.037882 a is is\n",
      "20 97.635350 a is is man man a man\n",
      "21 97.063509 a is is man a man\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-7a8f865c150b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m                                    \u001b[0mlabels_train\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx_label_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m                                    \u001b[0mlength\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0my_length\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m                                    \u001b[0mlength_train\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0my_length_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m                                })\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1122\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1124\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1125\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1321\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1304\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1305\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1306\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1308\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainCount = 0\n",
    "totalLoss = 0\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "for j in range(2000):\n",
    "    x_data, x_label, x_label_train, y_length, y_length_train = getMiniDataSets()\n",
    "    for i in range(29):\n",
    "        trainCount = trainCount + 1\n",
    "        \n",
    "        _,l,predict = sess.run([minimize, loss, sample], \n",
    "                               feed_dict={\n",
    "                                   inputs: x_data[i], \n",
    "                                   labels: x_label[i], \n",
    "                                   labels_train: x_label_train[i], \n",
    "                                   length: y_length[i],\n",
    "                                   length_train: y_length_train[i]\n",
    "                               })\n",
    "        \n",
    "        totalLoss += l\n",
    "    ran = randint(0,49)\n",
    "    log = \"%d %f %s\"%(j, totalLoss/trainCount, getStr(predict[ran]))\n",
    "    print(log)\n",
    "    if j%100==0 and j!=0:\n",
    "        saver = tf.train.Saver()\n",
    "        saver.save(sess, \"model\"+str(j)+\".ckpt\")\n",
    "        \n",
    "saver = tf.train.Saver()\n",
    "saver.save(sess, \"model\"+str(j)+\".ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
